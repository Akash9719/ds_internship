{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\users\\hp\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "#installing bs4 and requests for beautiful soup\n",
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1 - display all the header tags from ‘en.wikipedia.org/wiki/Main_Page’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing  necessory libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From today's featured article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Did you know ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In the news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On this day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Today's featured picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Other areas of Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wikipedia's sister projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wikipedia languages</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         headers\n",
       "0  From today's featured article\n",
       "1               Did you know ...\n",
       "2                    In the news\n",
       "3                    On this day\n",
       "4       Today's featured picture\n",
       "5       Other areas of Wikipedia\n",
       "6    Wikipedia's sister projects\n",
       "7            Wikipedia languages"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sending request to get the page data and souce code from web server for scraping\n",
    "page=requests.get('https://en.wikipedia.org/wiki/Main_Page')\n",
    "\n",
    "#getting page content\n",
    "soup=BeautifulSoup(page.content)\n",
    "\n",
    "#getting the text data using tags\n",
    "headers=soup.findAll('span',class_='mw-headline')\n",
    "\n",
    "display_headers=[ ] # empty list\n",
    "\n",
    "#using loop for append the headers one by one\n",
    "for i in headers:\n",
    "    display_headers.append(i.text)\n",
    "    \n",
    "#importing library to make the dataframe\n",
    "import pandas as pd\n",
    "\n",
    "#making dataframe\n",
    "heads= pd.DataFrame({})\n",
    "heads['headers']= display_headers\n",
    "\n",
    "#checking dataframe\n",
    "heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2 - IMDB’s Top rated 100 movies’ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movies name</th>\n",
       "      <th>year of release</th>\n",
       "      <th>IMDB rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>1994</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>1972</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Godfather: Part II</td>\n",
       "      <td>1974</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>2008</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>1957</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>North by Northwest</td>\n",
       "      <td>1959</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>A Clockwork Orange</td>\n",
       "      <td>1971</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Snatch</td>\n",
       "      <td>2000</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Le fabuleux destin d'Amélie Poulain</td>\n",
       "      <td>2001</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The Kid</td>\n",
       "      <td>1921</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            movies name year of release  IMDB rating\n",
       "0              The Shawshank Redemption            1994          9.3\n",
       "1                         The Godfather            1972          9.2\n",
       "2                The Godfather: Part II            1974          9.0\n",
       "3                       The Dark Knight            2008          9.0\n",
       "4                          12 Angry Men            1957          9.0\n",
       "..                                  ...             ...          ...\n",
       "95                   North by Northwest            1959          8.3\n",
       "96                   A Clockwork Orange            1971          8.3\n",
       "97                               Snatch            2000          8.3\n",
       "98  Le fabuleux destin d'Amélie Poulain            2001          8.3\n",
       "99                              The Kid            1921          8.3\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sending request to get the page data and souce code from web server for scraping\n",
    "page = requests.get(\"https://www.imdb.com/list/ls091520106/\")\n",
    "\n",
    "#getting page content\n",
    "soup=BeautifulSoup(page.content)\n",
    "\n",
    "#empty lists\n",
    "movie_name = []\n",
    "year = []\n",
    "rating = []\n",
    "\n",
    "#getting the names of the movies using its source code\n",
    "movie_data=soup.findAll(\"h3\", class_=\"lister-item-header\")\n",
    "for i in movie_data:\n",
    "    for j in i.find_all(\"a\"):\n",
    "        movie_name.append(j.text)\n",
    "        \n",
    "#getting the year of release of the movies using its source code\n",
    "years = soup.find_all(\"span\",class_=\"lister-item-year text-muted unbold\")\n",
    "for i in years:\n",
    "    y=i.text.replace('(','')\n",
    "    year.append(y.replace(')',''))\n",
    "    \n",
    "#getting the IMDB rating of the movies using its source code\n",
    "rate = soup.find_all(\"div\",class_=\"ipl-rating-star small\")\n",
    "for i in rate:\n",
    "      rating.append(float(i.text))\n",
    "        \n",
    "#making data frame for movies data\n",
    "import pandas as pd\n",
    "movies=pd.DataFrame({})\n",
    "movies['movies name']=movie_name\n",
    "movies['year of release']=year\n",
    "movies['IMDB rating']=rating\n",
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question-3 IMDB’s Top rated 100 Indian movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movies name</th>\n",
       "      <th>year of release</th>\n",
       "      <th>IMDB rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rang De Basanti</td>\n",
       "      <td>(2006)</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3 Idiots</td>\n",
       "      <td>(2009)</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Taare Zameen Par</td>\n",
       "      <td>(2007)</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dil Chahta Hai</td>\n",
       "      <td>(2001)</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Swades: We, the People</td>\n",
       "      <td>(2004)</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Wake Up Sid</td>\n",
       "      <td>(2009)</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Rangeela</td>\n",
       "      <td>(1995)</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Shatranj Ke Khilari</td>\n",
       "      <td>(1977)</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Pyaar Ka Punchnama</td>\n",
       "      <td>(2011)</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Ek Hasina Thi</td>\n",
       "      <td>(2004)</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               movies name year of release IMDB rating\n",
       "0          Rang De Basanti          (2006)         8.1\n",
       "1                 3 Idiots          (2009)         8.4\n",
       "2         Taare Zameen Par          (2007)         8.4\n",
       "3           Dil Chahta Hai          (2001)         8.1\n",
       "4   Swades: We, the People          (2004)         8.2\n",
       "..                     ...             ...         ...\n",
       "95             Wake Up Sid          (2009)         7.6\n",
       "96                Rangeela          (1995)         7.5\n",
       "97     Shatranj Ke Khilari          (1977)         7.7\n",
       "98      Pyaar Ka Punchnama          (2011)         7.6\n",
       "99           Ek Hasina Thi          (2004)         7.5\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sending request to get the page data and souce code from web server for scraping\n",
    "page = requests.get(\"https://www.imdb.com/list/ls009997493/\")\n",
    "\n",
    "#getting page content\n",
    "soup=BeautifulSoup(page.content)\n",
    "\n",
    "#empty lists\n",
    "movie_names = []\n",
    "year_of_release = []\n",
    "imdb_rating = []\n",
    "\n",
    "#getting the names of the movies using its source code\n",
    "movies=soup.find_all('h3',class_=\"lister-item-header\")\n",
    "for i in movies:\n",
    "    movie_names.append(i.a.text.replace('\\n',''))\n",
    "        \n",
    "#getting the year of release of the movies using its source code\n",
    "year=soup.find_all('span',class_='lister-item-year text-muted unbold')\n",
    "for i in year:\n",
    "    year_of_release.append(i.text)\n",
    "    \n",
    "#getting the IMDB rating of the movies using its source code\n",
    "rating=soup.find_all('div',class_='ipl-rating-star small')\n",
    "for i in rating:\n",
    "    imdb_rating.append(i.text.replace('\\n',''))\n",
    "        \n",
    "#making data frame for movies data\n",
    "movies=pd.DataFrame({})\n",
    "movies['movies name']=movie_names\n",
    "movies['year of release']=year_of_release\n",
    "movies['IMDB rating']=imdb_rating\n",
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Question 4 - scrap book name, author name, genre and book review of \n",
    "any 5 books from ‘www.bookpage.com’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Name</th>\n",
       "      <th>Author</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Facing the Mountain</td>\n",
       "      <td>Daniel James Brown</td>\n",
       "      <td>Nonfiction / History / American History</td>\n",
       "      <td>Imagine that you and your family have been tak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lucky Girl</td>\n",
       "      <td>Jamie Pacton</td>\n",
       "      <td>YA Fiction / YA</td>\n",
       "      <td>Seventeen-year-old Jane Belleweather has just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>On Juneteenth</td>\n",
       "      <td>Annette Gordon-Reed</td>\n",
       "      <td>Nonfiction / History / American History</td>\n",
       "      <td>Annette Gordon-Reed opens On Juneteenth by ref...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pawcasso</td>\n",
       "      <td>Remy Lai</td>\n",
       "      <td>Children's / Middle Grade</td>\n",
       "      <td>Pawcasso is a joyful graphic novel from acclai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Most Beautiful Girl in Cuba</td>\n",
       "      <td>Chanel Cleeton</td>\n",
       "      <td>Fiction / Historical Fiction</td>\n",
       "      <td>Through her popular historical novels, bestsel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Book Name               Author  \\\n",
       "0              Facing the Mountain   Daniel James Brown   \n",
       "1                       Lucky Girl         Jamie Pacton   \n",
       "2                    On Juneteenth  Annette Gordon-Reed   \n",
       "3                         Pawcasso             Remy Lai   \n",
       "4  The Most Beautiful Girl in Cuba       Chanel Cleeton   \n",
       "\n",
       "                                     Genre  \\\n",
       "0  Nonfiction / History / American History   \n",
       "1                          YA Fiction / YA   \n",
       "2  Nonfiction / History / American History   \n",
       "3                Children's / Middle Grade   \n",
       "4             Fiction / Historical Fiction   \n",
       "\n",
       "                                              Review  \n",
       "0  Imagine that you and your family have been tak...  \n",
       "1  Seventeen-year-old Jane Belleweather has just ...  \n",
       "2  Annette Gordon-Reed opens On Juneteenth by ref...  \n",
       "3  Pawcasso is a joyful graphic novel from acclai...  \n",
       "4  Through her popular historical novels, bestsel...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sending request to get the page data and souce code from web server for scraping\n",
    "page = requests.get(\"https://bookpage.com/reviews\")\n",
    "\n",
    "#getting page content\n",
    "soup=BeautifulSoup(page.content)\n",
    "\n",
    "#empty lists\n",
    "book_names = []\n",
    "author_name = []\n",
    "genre_of_book = []\n",
    "review_of_book = []\n",
    "\n",
    "#getting the names of the books using its source code\n",
    "books=soup.find_all('h4',class_='italic')\n",
    "for i in books:\n",
    "    book_names.append(i.a.text)\n",
    "        \n",
    "#getting the author name of the book using its source code\n",
    "authors=soup.find_all('p',class_='sans bold')\n",
    "for i in authors:\n",
    "    author_name.append(i.text.replace('\\n',''))\n",
    "    \n",
    "#getting the genere of the book using its source code\n",
    "genres=soup.find_all('p',class_='genre-links hidden-phone')\n",
    "for i in genres:\n",
    "    genre_of_book.append(i.text.replace('\\n',''))\n",
    "\n",
    "#getting the review of the book using its source code\n",
    "reviews=soup.find_all('p',class_='excerpt')\n",
    "for i in reviews:\n",
    "    review_of_book.append(i.text.replace('\\n',''))\n",
    "\n",
    "#making and show dataframe\n",
    "books= pd.DataFrame({})\n",
    "books['names']= book_names[:5]\n",
    "books['author']= author_name[:5]\n",
    "books['genre']= genre_of_book[:5]\n",
    "books['review']= review_of_book[:5]\n",
    "books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5 -  scrape cricket rankings from ‘www.icc-cricket.com’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Top 10 ODI teams in men’s cricket along with the records for matches, points and \n",
    "rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team</th>\n",
       "      <th>matches</th>\n",
       "      <th>points</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>32</td>\n",
       "      <td>3,793</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>England</td>\n",
       "      <td>28</td>\n",
       "      <td>3,244</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Australia</td>\n",
       "      <td>32</td>\n",
       "      <td>3,624</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India</td>\n",
       "      <td>25</td>\n",
       "      <td>2,459</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>27</td>\n",
       "      <td>2,524</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>30</td>\n",
       "      <td>2,740</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>30</td>\n",
       "      <td>2,523</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>32</td>\n",
       "      <td>2,657</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>17</td>\n",
       "      <td>1,054</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>7</td>\n",
       "      <td>336</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           team matches points ranking\n",
       "0   New Zealand      32  3,793     119\n",
       "1       England      28  3,244     116\n",
       "2     Australia      32  3,624     113\n",
       "3         India      25  2,459      98\n",
       "4  South Africa      27  2,524      93\n",
       "5      Pakistan      30  2,740      91\n",
       "6    Bangladesh      30  2,523      84\n",
       "7   West Indies      32  2,657      83\n",
       "8     Sri Lanka      17  1,054      62\n",
       "9   Afghanistan       7    336      48"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sending request to get the page data and souce code from web server for scraping\n",
    "page = requests.get(\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\")\n",
    "\n",
    "#getting page content\n",
    "soup=BeautifulSoup(page.content)\n",
    "\n",
    "#getting the names of the odi teams using its source code\n",
    "teams=soup.find_all('span',class_='u-hide-phablet')\n",
    "odi_teams=[ ]\n",
    "for i in teams:\n",
    "    odi_teams.append(i.text.replace('\\n',''))\n",
    "        \n",
    "#getting the number of matches using its source code\n",
    "match1=soup.find_all('td',class_='rankings-block__banner--matches')\n",
    "no_of_matches=[ ]\n",
    "for i in match1:\n",
    "    no_of_matches.append(i.text)\n",
    "        \n",
    "#getting the new list of match data using its source code\n",
    "match=soup.find_all('td',class_='table-body__cell u-center-text')\n",
    "new_list=[ ]\n",
    "for i in match:\n",
    "    new_list.append(i.text)\n",
    "\n",
    "#seprating the columns to get the point\n",
    "no_of_match=[]\n",
    "points=[]\n",
    "for i in range(0,len(new_list),2):\n",
    "    no_of_match.append(new_list[i])\n",
    "    points.append(new_list[i+1])\n",
    "    \n",
    "#getting the ranking of the odi team using its source code\n",
    "ranking=soup.find_all('td',class_='table-body__cell u-text-right rating')\n",
    "odi_ranking=[ ]\n",
    "for i in ranking:\n",
    "    odi_ranking.append(i.text.replace('\\n',''))\n",
    "\n",
    "#making dataframe using pandas\n",
    "import pandas as pd\n",
    "teams= pd.DataFrame({})\n",
    "teams['team']= odi_teams[:10]\n",
    "teams['matches']= no_of_match[:10]\n",
    "teams['points']= points[:10]\n",
    "teams['ranking'] = odi_ranking[:10]\n",
    "teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Top 10 ODI Batsmen in men along with the records of their team and rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Babar Azam</td>\n",
       "      <td>PAK</td>\n",
       "      <td>873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Virat Kohli</td>\n",
       "      <td>IND</td>\n",
       "      <td>844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rohit Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ross Taylor</td>\n",
       "      <td>NZ</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aaron Finch</td>\n",
       "      <td>AUS</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jonny Bairstow</td>\n",
       "      <td>ENG</td>\n",
       "      <td>775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>David Warner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Shai Hope</td>\n",
       "      <td>WI</td>\n",
       "      <td>758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kane Williamson</td>\n",
       "      <td>NZ</td>\n",
       "      <td>754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Quinton de Kock</td>\n",
       "      <td>SA</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Player                     Team Rating\n",
       "0       Babar Azam  PAK                        873\n",
       "1      Virat Kohli                      IND    844\n",
       "2     Rohit Sharma                      IND    813\n",
       "3      Ross Taylor                       NZ    801\n",
       "4      Aaron Finch                      AUS    779\n",
       "5   Jonny Bairstow                      ENG    775\n",
       "6     David Warner                      AUS    762\n",
       "7        Shai Hope                       WI    758\n",
       "8  Kane Williamson                       NZ    754\n",
       "9  Quinton de Kock                       SA    747"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sending request to get the page data and souce code from web server for scraping\n",
    "page=requests.get('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting')\n",
    "\n",
    "#getting page content\n",
    "soup=BeautifulSoup(page.content)\n",
    "\n",
    "#making empty lists\n",
    "players=[]\n",
    "team_name=[]\n",
    "rating=[]\n",
    "\n",
    "#get the data of ranking of the team in odi using their tags\n",
    "for i in soup.find_all(\"div\",class_='rankings-block__banner--name-large'): \n",
    "    players.append(i.text)\n",
    "\n",
    "#get the data of ranking of the team in odi using their tags\n",
    "for i in soup.find_all(\"div\",class_='rankings-block__banner--nationality'):\n",
    "    team_name.append(i.text.replace(\"\\n\",\"\"))\n",
    "\n",
    "#getting the data from the web page by theire source code\n",
    "for i in soup.find_all(\"td\",class_='table-body__cell rankings-table__name name'):\n",
    "    for j in i.find_all('a'):\n",
    "        players.append(j.text)\n",
    "\n",
    "#getting the data from the web page by theire source code\n",
    "for i in soup.find_all(\"span\",class_='table-body__logo-text'): \n",
    "    team_name.append(i.text)\n",
    "\n",
    "#getting the rating of player from the web page by theire source code    \n",
    "for i in soup.find_all(\"div\",class_='rankings-block__banner--rating'): \n",
    "    rating.append(i.text)\n",
    "\n",
    "#getting the rating of player from the web page by theire source code \n",
    "for i in soup.find_all(\"td\",class_='table-body__cell rating'): \n",
    "    rating.append(i.text)\n",
    "    \n",
    "#making dataframe using pandas\n",
    "Batsmen=pd.DataFrame({})\n",
    "Batsmen['Player']=players[:10]\n",
    "Batsmen['Team']=team_name[:10]\n",
    "Batsmen['Rating']=rating[:10]\n",
    "Batsmen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii) Top 10 ODI bowlers along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trent Boult</td>\n",
       "      <td>NZ</td>\n",
       "      <td>737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Josh Hazlewood</td>\n",
       "      <td>AUS</td>\n",
       "      <td>709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mujeeb Ur Rahman</td>\n",
       "      <td>AFG</td>\n",
       "      <td>708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chris Woakes</td>\n",
       "      <td>ENG</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mehedi Hasan</td>\n",
       "      <td>BAN</td>\n",
       "      <td>692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Matt Henry</td>\n",
       "      <td>NZ</td>\n",
       "      <td>691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jasprit Bumrah</td>\n",
       "      <td>IND</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mitchell Starc</td>\n",
       "      <td>AUS</td>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shakib Al Hasan</td>\n",
       "      <td>BAN</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kagiso Rabada</td>\n",
       "      <td>SA</td>\n",
       "      <td>646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Player                    Team Rating\n",
       "0       Trent Boult  NZ                        737\n",
       "1    Josh Hazlewood                     AUS    709\n",
       "2  Mujeeb Ur Rahman                     AFG    708\n",
       "3      Chris Woakes                     ENG    700\n",
       "4      Mehedi Hasan                     BAN    692\n",
       "5        Matt Henry                      NZ    691\n",
       "6    Jasprit Bumrah                     IND    679\n",
       "7    Mitchell Starc                     AUS    652\n",
       "8   Shakib Al Hasan                     BAN    650\n",
       "9     Kagiso Rabada                      SA    646"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sending request to get the page data and souce code from web server for scraping\n",
    "page = requests.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\")\n",
    "\n",
    "#getting page content\n",
    "soup=BeautifulSoup(page.content)\n",
    "\n",
    "#get the name of the player of the team in odi using their tags\n",
    "players=[]\n",
    "for i in soup.find_all(\"div\",class_='rankings-block__banner--name-large'):\n",
    "    players.append(i.text)\n",
    "    \n",
    "#making list of name of the player of the team in odi using their tags\n",
    "for i in soup.find_all(\"td\",class_='table-body__cell rankings-table__name name'):\n",
    "    for j in i.find_all('a'):\n",
    "        players.append(j.text)\n",
    "\n",
    "#getting the name of the team from the web page by theire source code\n",
    "team_name=[]\n",
    "for i in soup.find_all(\"div\",class_='rankings-block__banner--nationality'):\n",
    "    team_name.append(i.text.replace(\"\\n\",\"\"))\n",
    "\n",
    "#making list of the name of the team from the web page by theire source code\n",
    "for i in soup.find_all(\"span\",class_='table-body__logo-text'): \n",
    "    team_name.append(i.text)\n",
    "    \n",
    "#getting the rating of player from the web page by theire source code    \n",
    "rating=[]\n",
    "for i in soup.find_all(\"div\",class_='rankings-block__banner--rating'):\n",
    "    rating.append(i.text)\n",
    "\n",
    "#making list of rating of player from the web page by theire source code \n",
    "for i in soup.find_all(\"td\",class_='table-body__cell rating'):\n",
    "    rating.append(i.text)\n",
    "    \n",
    "#making dataframe using pandas\n",
    "bowlers=pd.DataFrame({})\n",
    "bowlers['Player']=players[:10]\n",
    "bowlers['Team']=team_name[:10]\n",
    "bowlers['Rating']=rating[:10]\n",
    "bowlers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6 - Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Top 10 ODI teams in women’s cricket along with the records for matches, points \n",
    "and rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team</th>\n",
       "      <th>matches</th>\n",
       "      <th>points</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Australia</td>\n",
       "      <td>32</td>\n",
       "      <td>3,793</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>England</td>\n",
       "      <td>28</td>\n",
       "      <td>3,244</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>32</td>\n",
       "      <td>3,624</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India</td>\n",
       "      <td>25</td>\n",
       "      <td>2,459</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>27</td>\n",
       "      <td>2,524</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>30</td>\n",
       "      <td>2,740</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>30</td>\n",
       "      <td>2,523</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>32</td>\n",
       "      <td>2,657</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>17</td>\n",
       "      <td>1,054</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           team matches points ranking\n",
       "0     Australia      32  3,793     119\n",
       "1       England      28  3,244     117\n",
       "2  South Africa      32  3,624     113\n",
       "3         India      25  2,459      92\n",
       "4   New Zealand      27  2,524      85\n",
       "5   West Indies      30  2,740      75\n",
       "6      Pakistan      30  2,523      61\n",
       "7    Bangladesh      32  2,657      47\n",
       "8     Sri Lanka      17  1,054      13"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sending request to get the page data and souce code from web server for scraping\n",
    "page = requests.get(\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\")\n",
    "\n",
    "#getting page content\n",
    "soup=BeautifulSoup(page.content)\n",
    "\n",
    "#getting the name of the team in odi using their tags\n",
    "odi_teams=[ ]\n",
    "teams=soup.find_all('span',class_='u-hide-phablet')\n",
    "\n",
    "#splitting the team names\n",
    "for i in teams:\n",
    "    odi_teams.append(i.text.replace('\\n',''))\n",
    "    \n",
    "#making list of matches of the team in odi using their tags\n",
    "no_of_matches=[ ]\n",
    "match1=soup.find_all('td',class_='rankings-block__banner--matches')\n",
    "match=soup.find_all('td',class_='table-body__cell u-center-text')\n",
    "\n",
    "#splitting the match name\n",
    "new_list=[ ]\n",
    "for i in match:\n",
    "    new_list.append(i.text)\n",
    "\n",
    "#getting the ranking of the tean from the web page by theire source code\n",
    "odi_ranking=[ ]\n",
    "ranking=soup.find_all('td',class_='table-body__cell u-text-right rating')\n",
    "ranking\n",
    "\n",
    "#splitting the ranking of team\n",
    "for i in ranking:\n",
    "    odi_ranking.append(i.text.replace('\\n',''))\n",
    "\n",
    "#making dataframe using pandas\n",
    "teams= pd.DataFrame({})\n",
    "teams['team']= odi_teams[:9]\n",
    "teams['matches']= no_of_match[:9]\n",
    "teams['points']= points[:9]\n",
    "teams['ranking'] = odi_ranking[:9]\n",
    "teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Top 10 women’s ODI players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alyssa Healy</td>\n",
       "      <td>AUS</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mithali Raj</td>\n",
       "      <td>IND</td>\n",
       "      <td>738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tammy Beaumont</td>\n",
       "      <td>ENG</td>\n",
       "      <td>728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amy Satterthwaite</td>\n",
       "      <td>NZ</td>\n",
       "      <td>717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Smriti Mandhana</td>\n",
       "      <td>IND</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Meg Lanning</td>\n",
       "      <td>AUS</td>\n",
       "      <td>699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Beth Mooney</td>\n",
       "      <td>AUS</td>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Heather Knight</td>\n",
       "      <td>ENG</td>\n",
       "      <td>674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Laura Wolvaardt</td>\n",
       "      <td>SA</td>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Rachael Haynes</td>\n",
       "      <td>AUS</td>\n",
       "      <td>668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Player Team Rating\n",
       "0       Alyssa Healy  AUS    750\n",
       "1        Mithali Raj  IND    738\n",
       "2     Tammy Beaumont  ENG    728\n",
       "3  Amy Satterthwaite   NZ    717\n",
       "4    Smriti Mandhana  IND    710\n",
       "5        Meg Lanning  AUS    699\n",
       "6        Beth Mooney  AUS    690\n",
       "7     Heather Knight  ENG    674\n",
       "8    Laura Wolvaardt   SA    672\n",
       "9     Rachael Haynes  AUS    668"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sending request to get the page data and souce code from web server for scraping\n",
    "page = requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting')\n",
    "\n",
    "#getting page content\n",
    "soup=BeautifulSoup(page.content)\n",
    "\n",
    "#get the data of ranking of the team in odi using their tags\n",
    "for i in soup.find_all(\"div\",class_='rankings-block__banner--name-large'):\n",
    "    players.append(i.text)\n",
    "    \n",
    "#get the data of ranking of the team in odi using their tags\n",
    "players=[]\n",
    "for i in soup.find_all(\"td\",class_='table-body__cell rankings-table__name name'):\n",
    "    for j in i.find_all('a'):\n",
    "        players.append(j.text)\n",
    "\n",
    "#getting the data from the web page by theire source code\n",
    "for i in soup.find_all(\"div\",class_='rankings-block__banner--nationality'):\n",
    "    team_name.append(i.text.replace(\"\\n\",\"\"))\n",
    "\n",
    "#getting the data from the web page by theire source code\n",
    "team_name=[]\n",
    "for i in soup.find_all(\"span\",class_='table-body__logo-text'):\n",
    "    team_name.append(i.text)\n",
    "\n",
    "#getting the rating of player from the web page by theire source code    \n",
    "for i in soup.find_all(\"div\",class_='rankings-block__banner--rating'):\n",
    "    rating.append(i.text)\n",
    "\n",
    "#getting the rating of player from the web page by theire source code \n",
    "rating=[]\n",
    "for i in soup.find_all(\"td\",class_='table-body__cell rating'):\n",
    "    rating.append(i.text)\n",
    "    \n",
    "#making dataframe using pandas\n",
    "top_players=pd.DataFrame({})\n",
    "top_players['Player']=players[:10]\n",
    "top_players['Team']=team_name[:10]\n",
    "top_players['Rating']=rating[:10]\n",
    "top_players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii)Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jess Jonassen</td>\n",
       "      <td>AUS</td>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jhulan Goswami</td>\n",
       "      <td>IND</td>\n",
       "      <td>727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Megan Schutt</td>\n",
       "      <td>AUS</td>\n",
       "      <td>717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sophie Ecclestone</td>\n",
       "      <td>ENG</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Shabnim Ismail</td>\n",
       "      <td>SA</td>\n",
       "      <td>688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Katherine Brunt</td>\n",
       "      <td>ENG</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ayabonga Khaka</td>\n",
       "      <td>SA</td>\n",
       "      <td>643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Anya Shrubsole</td>\n",
       "      <td>ENG</td>\n",
       "      <td>598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kate Cross</td>\n",
       "      <td>ENG</td>\n",
       "      <td>589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Player                     Team Rating\n",
       "0      Jess Jonassen  AUS                        760\n",
       "1     Jhulan Goswami                      IND    727\n",
       "2       Megan Schutt                      AUS    717\n",
       "3     Marizanne Kapp                       SA    715\n",
       "4  Sophie Ecclestone                      ENG    701\n",
       "5     Shabnim Ismail                       SA    688\n",
       "6    Katherine Brunt                      ENG    666\n",
       "7     Ayabonga Khaka                       SA    643\n",
       "8     Anya Shrubsole                      ENG    598\n",
       "9         Kate Cross                      ENG    589"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sending request to get the page data and souce code from web server for scraping\n",
    "page = requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/bowling')\n",
    "\n",
    "#getting page content\n",
    "soup=BeautifulSoup(page.content)\n",
    "\n",
    "#get the name of the player of the team in odi using their tags\n",
    "players=[]\n",
    "for i in soup.find_all(\"div\",class_='rankings-block__banner--name-large'):\n",
    "    players.append(i.text)\n",
    "    \n",
    "#making list of name of the player of the team in odi using their tags\n",
    "for i in soup.find_all(\"td\",class_='table-body__cell rankings-table__name name'):\n",
    "    for j in i.find_all('a'):\n",
    "        players.append(j.text)\n",
    "\n",
    "#getting the name of the team from the web page by theire source code\n",
    "team_name=[]\n",
    "for i in soup.find_all(\"div\",class_='rankings-block__banner--nationality'):\n",
    "    team_name.append(i.text.replace(\"\\n\",\"\"))\n",
    "\n",
    "#making list of the name of the team from the web page by theire source code\n",
    "for i in soup.find_all(\"span\",class_='table-body__logo-text'): \n",
    "    team_name.append(i.text)\n",
    "    \n",
    "#getting the rating of player from the web page by theire source code    \n",
    "rating=[]\n",
    "for i in soup.find_all(\"div\",class_='rankings-block__banner--rating'):\n",
    "    rating.append(i.text)\n",
    "\n",
    "#making list of rating of player from the web page by theire source code \n",
    "for i in soup.find_all(\"td\",class_='table-body__cell rating'):\n",
    "    rating.append(i.text)\n",
    "    \n",
    "#making dataframe using pandas\n",
    "bowlers=pd.DataFrame({})\n",
    "bowlers['Player']=players[:10]\n",
    "bowlers['Team']=team_name[:10]\n",
    "bowlers['Rating']=rating[:10]\n",
    "bowlers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7 - scrape details of all the mobile phones under Rs. 20,000 \n",
    "listed on Amazon.in. The scraped data should include Product Name, Price, Image URL \n",
    "and Average Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='www.amazon.in', port=443): Max retries exceeded with url: /s?k=Mobile+phones+under+20000&ref=nb_sb_noss_2 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002063599BB80>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             conn = connection.create_connection(\n\u001b[0m\u001b[0;32m    160\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    917\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    669\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m    977\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sock\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;31m# Add certificate verification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m         \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSocketError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m             raise NewConnectionError(\n\u001b[0m\u001b[0;32m    172\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x000002063599BB80>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 726\u001b[1;33m             retries = retries.increment(\n\u001b[0m\u001b[0;32m    727\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.amazon.in', port=443): Max retries exceeded with url: /s?k=Mobile+phones+under+20000&ref=nb_sb_noss_2 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002063599BB80>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-e8cde48e9d42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#sending request to get the page data and souce code from web server for scraping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://www.amazon.in/s?k=Mobile+phones+under+20000&ref=nb_sb_noss_2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#getting page content\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    528\u001b[0m         }\n\u001b[0;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    514\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='www.amazon.in', port=443): Max retries exceeded with url: /s?k=Mobile+phones+under+20000&ref=nb_sb_noss_2 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002063599BB80>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))"
     ]
    }
   ],
   "source": [
    "#sending request to get the page data and souce code from web server for scraping\n",
    "page = requests.get(\"https://www.amazon.in/s?k=Mobile+phones+under+20000&ref=nb_sb_noss_2\")\n",
    "\n",
    "#getting page content\n",
    "soup=BeautifulSoup(page.content)\n",
    "\n",
    "#making empty list\n",
    "phone_names = [] \n",
    "phone_price = []   \n",
    "phone_rating = []  \n",
    "phone_image = []  \n",
    "\n",
    "#getting the names of the mobile phones\n",
    "for i in soup.find_all(\"span\",class_=\"a-size-medium a-color-base a-text-normal\"):\n",
    "    product_name.append(i.text)\n",
    "\n",
    "#getting the prices of the mobiles from their source code\n",
    "for i in soup.find_all(\"span\",class_=\"a-price-whole\"):\n",
    "    price.append(i.text)\n",
    "    \n",
    "#getting the rating of the mobiles\n",
    "for i in soup.find_all(\"span\",class_=\"a-icon-alt\"):\n",
    "    rating.append(i.text)\n",
    "\n",
    "#getting the image urls of the mobile\n",
    "for i in soup.find_all(\"img\",class_=\"s-image\"):\n",
    "    img_url.append(i.get(\"src\"))\n",
    "    \n",
    "#making the dataframe\n",
    "phone= pd.DataFrame({})\n",
    "phone['name']= phone_names\n",
    "phone['price']= phone_price\n",
    "phone['rating']= phone_rating\n",
    "phone['image_url']= phone_image\n",
    "\n",
    "phone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8 - local weather from the National \n",
    "Weather Service website of USA, https://www.weather.gov/ for the city, San \n",
    "Francisco. You need to extract data about 7 day extended forecast display for the city. \n",
    "The data should include period, short description, temperature and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period</th>\n",
       "      <th>temperature</th>\n",
       "      <th>short_desc</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Today</td>\n",
       "      <td>High: 66 °F</td>\n",
       "      <td>Partly sunny</td>\n",
       "      <td>Partly sunny, then gradually becoming sunny, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tonight</td>\n",
       "      <td>Low: 50 °F</td>\n",
       "      <td>Partly cloudy</td>\n",
       "      <td>Partly cloudy, with a low around 50. Light and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wednesday</td>\n",
       "      <td>High: 66 °F</td>\n",
       "      <td>Mostly sunny</td>\n",
       "      <td>Mostly sunny, with a high near 66. East wind a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wednesday Night</td>\n",
       "      <td>Low: 51 °F</td>\n",
       "      <td>Mostly cloudy</td>\n",
       "      <td>Mostly cloudy, with a low around 51. West wind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>High: 63 °F</td>\n",
       "      <td>Partly sunny</td>\n",
       "      <td>Partly sunny, with a high near 63. Light and v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thursday Night</td>\n",
       "      <td>Low: 52 °F</td>\n",
       "      <td>A 30 percent chance of rain</td>\n",
       "      <td>A 30 percent chance of rain, mainly before 4am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Friday</td>\n",
       "      <td>High: 63 °F</td>\n",
       "      <td>A 20 percent chance of rain before 10am.  Most...</td>\n",
       "      <td>A 20 percent chance of rain before 10am.  Most...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Friday Night</td>\n",
       "      <td>Low: 52 °F</td>\n",
       "      <td>Mostly cloudy</td>\n",
       "      <td>Mostly cloudy, with a low around 52.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Saturday</td>\n",
       "      <td>High: 64 °F</td>\n",
       "      <td>Mostly sunny</td>\n",
       "      <td>Mostly sunny, with a high near 64.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            period  temperature  \\\n",
       "0            Today  High: 66 °F   \n",
       "1          Tonight   Low: 50 °F   \n",
       "2        Wednesday  High: 66 °F   \n",
       "3  Wednesday Night   Low: 51 °F   \n",
       "4         Thursday  High: 63 °F   \n",
       "5   Thursday Night   Low: 52 °F   \n",
       "6           Friday  High: 63 °F   \n",
       "7     Friday Night   Low: 52 °F   \n",
       "8         Saturday  High: 64 °F   \n",
       "\n",
       "                                          short_desc  \\\n",
       "0                                       Partly sunny   \n",
       "1                                      Partly cloudy   \n",
       "2                                       Mostly sunny   \n",
       "3                                      Mostly cloudy   \n",
       "4                                       Partly sunny   \n",
       "5                        A 30 percent chance of rain   \n",
       "6  A 20 percent chance of rain before 10am.  Most...   \n",
       "7                                      Mostly cloudy   \n",
       "8                                       Mostly sunny   \n",
       "\n",
       "                                         description  \n",
       "0  Partly sunny, then gradually becoming sunny, w...  \n",
       "1  Partly cloudy, with a low around 50. Light and...  \n",
       "2  Mostly sunny, with a high near 66. East wind a...  \n",
       "3  Mostly cloudy, with a low around 51. West wind...  \n",
       "4  Partly sunny, with a high near 63. Light and v...  \n",
       "5  A 30 percent chance of rain, mainly before 4am...  \n",
       "6  A 20 percent chance of rain before 10am.  Most...  \n",
       "7               Mostly cloudy, with a low around 52.  \n",
       "8                 Mostly sunny, with a high near 64.  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sending request to get the page data and souce code from web server for scraping\n",
    "page = requests.get(\"https://forecast.weather.gov/MapClick.php?textField1=37.78&textField2=-122.42#.YJJVsLUzZPY\")\n",
    "\n",
    "#getting page content\n",
    "soup=BeautifulSoup(page.content)\n",
    "\n",
    "#geting the time period og the whether\n",
    "period=soup.findAll('div',class_='col-sm-2 forecast-label')\n",
    "time_period=[ ]\n",
    "for i in period:\n",
    "    time_period.append(i.text)\n",
    "\n",
    "#getting short discription about the whether\n",
    "short_desc=soup.findAll('div',class_='col-sm-10 forecast-text')\n",
    "short_description=[]\n",
    "for i in short_desc:\n",
    "    short_description.append(i.text.split(',')[0])\n",
    "\n",
    "# Scrape temperature\n",
    "temperature=[]\n",
    "for i in soup.find_all('p',attrs={'short-desc'}):\n",
    "        if i.next_sibling is not None:\n",
    "            temperature.append(i.next_sibling.text)\n",
    "        else:\n",
    "            temperature.append(' ')\n",
    "            \n",
    "#getting full description of whether\n",
    "description=soup.find_all('div',class_='col-sm-10 forecast-text')\n",
    "sf_description=[]\n",
    "for i in description:\n",
    "    sf_description.append(i.text)\n",
    "\n",
    "#making dataframe with scraped data \n",
    "weather= pd.DataFrame({})\n",
    "weather['period']= time_period[:9]\n",
    "weather['temperature']= temperature[:9]\n",
    "weather['short_desc']= short_description[:9]\n",
    "weather['description']= sf_description[:9]\n",
    "\n",
    "weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 9. Write a python program to scrape fresher job listings from ‘https://internshala.com/’. It should include job title, \n",
    "company name, CTC, and apply date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>ctc</th>\n",
       "      <th>Apply_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business Development Executive</td>\n",
       "      <td>Decathlon Sports India</td>\n",
       "      <td>3 - 4 LPA</td>\n",
       "      <td>29 Nov' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Customer Relationship Executive - Affordable H...</td>\n",
       "      <td>NSE Academy</td>\n",
       "      <td>3.5 - 4.5 LPA</td>\n",
       "      <td>18 Nov' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Capital Market - Dealer &amp; Senior Dealer</td>\n",
       "      <td>NSE Academy</td>\n",
       "      <td>3.5 - 4.5 LPA</td>\n",
       "      <td>18 Nov' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Capital Market - Dealer &amp; Senior Dealer</td>\n",
       "      <td>NSE Academy</td>\n",
       "      <td>3.5 - 4.5 LPA</td>\n",
       "      <td>18 Nov' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Associate Graphic Designer</td>\n",
       "      <td>Frizzon Productions</td>\n",
       "      <td>3 - 4 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Full Stack Developer</td>\n",
       "      <td>Arishti CyberTech Private Limited</td>\n",
       "      <td>3.4 - 5 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Entry Specialist</td>\n",
       "      <td>BluCognition Private Limited</td>\n",
       "      <td>3 - 4 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TECHNICAL CONTENT WRITER</td>\n",
       "      <td>Aditya Farrad Production</td>\n",
       "      <td>3 - 3.5 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Engineer - Trainee</td>\n",
       "      <td>RMgX</td>\n",
       "      <td>4.5 - 4.8 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sales Associate</td>\n",
       "      <td>Yellow Oak Realty</td>\n",
       "      <td>3 - 3.25 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Operations Executive</td>\n",
       "      <td>MentorBoxx</td>\n",
       "      <td>4 - 7 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Business Development Executive</td>\n",
       "      <td>Kazam EV Tech</td>\n",
       "      <td>3 - 5 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Node.js Developer</td>\n",
       "      <td>Infoware</td>\n",
       "      <td>3.5 - 5 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Growth Marketer</td>\n",
       "      <td>Unbound</td>\n",
       "      <td>4 - 6 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Manager (Marketing &amp; Operations)</td>\n",
       "      <td>Kalaghar</td>\n",
       "      <td>3 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Technical Editor</td>\n",
       "      <td>Aditya Farrad Production</td>\n",
       "      <td>3 - 3.5 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Junior Full Stack Developer</td>\n",
       "      <td>Alice Camera Private Limited</td>\n",
       "      <td>5 - 7 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Associate Manager (Partnerships &amp; Fundraising)</td>\n",
       "      <td>Cocogreen Foods LLP</td>\n",
       "      <td>3.6 - 4.8 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Customer Service Executive</td>\n",
       "      <td>WhataPortrait.com</td>\n",
       "      <td>3 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Junior Backend Developer</td>\n",
       "      <td>Nodeberry Private Limited</td>\n",
       "      <td>4.5 - 7 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Software Engineer- I</td>\n",
       "      <td>AssetPlus</td>\n",
       "      <td>7 - 8 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Personal Assistant</td>\n",
       "      <td>Zotezo Com Enterprise Private Limited</td>\n",
       "      <td>3 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Video Editor</td>\n",
       "      <td>Dreamfoot</td>\n",
       "      <td>3 - 4 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Web Developer (MERN/MEAN/Full Stack)</td>\n",
       "      <td>ISkylar Technologies</td>\n",
       "      <td>3.5 - 4.5 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Recruitment Consultant</td>\n",
       "      <td>CareersAhead</td>\n",
       "      <td>3 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Business Development Executive</td>\n",
       "      <td>InfyBytes AI Labs Private Limited</td>\n",
       "      <td>3 - 4 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Counselor</td>\n",
       "      <td>UFaber Edutech</td>\n",
       "      <td>3 - 3.6 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Backend Developer</td>\n",
       "      <td>Homejam</td>\n",
       "      <td>6 - 8 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Customer Service Manager</td>\n",
       "      <td>Nutty Yogi</td>\n",
       "      <td>3 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Social Media Strategist And Copywriter</td>\n",
       "      <td>Internshala Trainings</td>\n",
       "      <td>5 - 6.8 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Node.js Developer</td>\n",
       "      <td>Konverse AI</td>\n",
       "      <td>4 - 6 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Full Stack Developer (MERN)</td>\n",
       "      <td>Cliffex Software Solutions</td>\n",
       "      <td>3 - 5 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Business Development Associate</td>\n",
       "      <td>Civilsdaily</td>\n",
       "      <td>3 - 4 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Content Editor &amp; Proofreader</td>\n",
       "      <td>CBD Marketing Solutions</td>\n",
       "      <td>3 - 3.2 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Auto Loan Advisor &amp; Sales Consultant</td>\n",
       "      <td>CarJasoos</td>\n",
       "      <td>3 - 4 LPA</td>\n",
       "      <td>16 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Junior Software Developer</td>\n",
       "      <td>Saral Technologies</td>\n",
       "      <td>4 - 5 LPA</td>\n",
       "      <td>15 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Python Developer</td>\n",
       "      <td>Ignis Tech Solutions</td>\n",
       "      <td>3 - 5 LPA</td>\n",
       "      <td>15 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Business Development Executive</td>\n",
       "      <td>Possiblers</td>\n",
       "      <td>3 - 3.5 LPA</td>\n",
       "      <td>15 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Full Stack Developer</td>\n",
       "      <td>Necesario Innovations Private Limited</td>\n",
       "      <td>3.2 - 4.8 LPA</td>\n",
       "      <td>15 Dec' 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Crypto Tech Recruiter</td>\n",
       "      <td>Wono Inc</td>\n",
       "      <td>5.3 - 6.1 LPA</td>\n",
       "      <td>15 Dec' 21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0                     Business Development Executive    \n",
       "1   Customer Relationship Executive - Affordable H...   \n",
       "2            Capital Market - Dealer & Senior Dealer    \n",
       "3            Capital Market - Dealer & Senior Dealer    \n",
       "4                         Associate Graphic Designer    \n",
       "5                               Full Stack Developer    \n",
       "6                              Data Entry Specialist    \n",
       "7                           TECHNICAL CONTENT WRITER    \n",
       "8                                 Engineer - Trainee    \n",
       "9                                    Sales Associate    \n",
       "10                              Operations Executive    \n",
       "11                    Business Development Executive    \n",
       "12                                 Node.js Developer    \n",
       "13                                   Growth Marketer    \n",
       "14                  Manager (Marketing & Operations)    \n",
       "15                                  Technical Editor    \n",
       "16                       Junior Full Stack Developer    \n",
       "17    Associate Manager (Partnerships & Fundraising)    \n",
       "18                        Customer Service Executive    \n",
       "19                          Junior Backend Developer    \n",
       "20                              Software Engineer- I    \n",
       "21                                Personal Assistant    \n",
       "22                                      Video Editor    \n",
       "23              Web Developer (MERN/MEAN/Full Stack)    \n",
       "24                            Recruitment Consultant    \n",
       "25                    Business Development Executive    \n",
       "26                                         Counselor    \n",
       "27                                 Backend Developer    \n",
       "28                          Customer Service Manager    \n",
       "29            Social Media Strategist And Copywriter    \n",
       "30                                 Node.js Developer    \n",
       "31                       Full Stack Developer (MERN)    \n",
       "32                    Business Development Associate    \n",
       "33                      Content Editor & Proofreader    \n",
       "34              Auto Loan Advisor & Sales Consultant    \n",
       "35                         Junior Software Developer    \n",
       "36                                  Python Developer    \n",
       "37                    Business Development Executive    \n",
       "38                              Full Stack Developer    \n",
       "39                             Crypto Tech Recruiter    \n",
       "\n",
       "                                  company            ctc  Apply_Date  \n",
       "0                  Decathlon Sports India      3 - 4 LPA  29 Nov' 21  \n",
       "1                             NSE Academy  3.5 - 4.5 LPA  18 Nov' 21  \n",
       "2                             NSE Academy  3.5 - 4.5 LPA  18 Nov' 21  \n",
       "3                             NSE Academy  3.5 - 4.5 LPA  18 Nov' 21  \n",
       "4                     Frizzon Productions      3 - 4 LPA  16 Dec' 21  \n",
       "5       Arishti CyberTech Private Limited    3.4 - 5 LPA  16 Dec' 21  \n",
       "6            BluCognition Private Limited      3 - 4 LPA  16 Dec' 21  \n",
       "7                Aditya Farrad Production    3 - 3.5 LPA  16 Dec' 21  \n",
       "8                                    RMgX  4.5 - 4.8 LPA  16 Dec' 21  \n",
       "9                       Yellow Oak Realty   3 - 3.25 LPA  16 Dec' 21  \n",
       "10                             MentorBoxx      4 - 7 LPA  16 Dec' 21  \n",
       "11                          Kazam EV Tech      3 - 5 LPA  16 Dec' 21  \n",
       "12                               Infoware    3.5 - 5 LPA  16 Dec' 21  \n",
       "13                                Unbound      4 - 6 LPA  16 Dec' 21  \n",
       "14                               Kalaghar          3 LPA  16 Dec' 21  \n",
       "15               Aditya Farrad Production    3 - 3.5 LPA  16 Dec' 21  \n",
       "16           Alice Camera Private Limited      5 - 7 LPA  16 Dec' 21  \n",
       "17                    Cocogreen Foods LLP  3.6 - 4.8 LPA  16 Dec' 21  \n",
       "18                      WhataPortrait.com          3 LPA  16 Dec' 21  \n",
       "19              Nodeberry Private Limited    4.5 - 7 LPA  16 Dec' 21  \n",
       "20                              AssetPlus      7 - 8 LPA  16 Dec' 21  \n",
       "21  Zotezo Com Enterprise Private Limited          3 LPA  16 Dec' 21  \n",
       "22                              Dreamfoot      3 - 4 LPA  16 Dec' 21  \n",
       "23                   ISkylar Technologies  3.5 - 4.5 LPA  16 Dec' 21  \n",
       "24                           CareersAhead          3 LPA  16 Dec' 21  \n",
       "25      InfyBytes AI Labs Private Limited      3 - 4 LPA  16 Dec' 21  \n",
       "26                         UFaber Edutech    3 - 3.6 LPA  16 Dec' 21  \n",
       "27                                Homejam      6 - 8 LPA  16 Dec' 21  \n",
       "28                             Nutty Yogi          3 LPA  16 Dec' 21  \n",
       "29                  Internshala Trainings    5 - 6.8 LPA  16 Dec' 21  \n",
       "30                            Konverse AI      4 - 6 LPA  16 Dec' 21  \n",
       "31             Cliffex Software Solutions      3 - 5 LPA  16 Dec' 21  \n",
       "32                            Civilsdaily      3 - 4 LPA  16 Dec' 21  \n",
       "33                CBD Marketing Solutions    3 - 3.2 LPA  16 Dec' 21  \n",
       "34                              CarJasoos      3 - 4 LPA  16 Dec' 21  \n",
       "35                     Saral Technologies      4 - 5 LPA  15 Dec' 21  \n",
       "36                   Ignis Tech Solutions      3 - 5 LPA  15 Dec' 21  \n",
       "37                             Possiblers    3 - 3.5 LPA  15 Dec' 21  \n",
       "38  Necesario Innovations Private Limited  3.2 - 4.8 LPA  15 Dec' 21  \n",
       "39                               Wono Inc  5.3 - 6.1 LPA  15 Dec' 21  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sending request to get the page data and souce code from web server for scraping\n",
    "page = requests.get(\"https://internshala.com/fresher-jobs\")\n",
    "\n",
    "#getting page content\n",
    "soup=BeautifulSoup(page.content)\n",
    "\n",
    "#geting the titles of the job from the source code\n",
    "titles=soup.find_all('div',class_=\"heading_4_5 profile\")\n",
    "job_titles=[ ]\n",
    "for i in titles:\n",
    "    job_titles.append(i.text.replace('\\n',''))\n",
    "    \n",
    "#geting the namw of the companies from the source code\n",
    "companies=soup.find_all('a',class_='link_display_like_text')\n",
    "company_names=[ ]\n",
    "for i in companies:\n",
    "    company_names.append(i.text.strip())\n",
    "\n",
    "#geting the info of the jobs from the source code\n",
    "info=soup.find_all('div',class_=\"item_body\")\n",
    "job_info = []\n",
    "for i in info:\n",
    "    job_info.append(i.text.replace(\"\\n\",\"\").replace('Immediately',''))\n",
    "            \n",
    "#geting the apply date of the jobs from the source code till when you can apply\n",
    "apply_date = []\n",
    "for i in range(2,len(job_info),3):\n",
    "    apply_date.append(job_info[i].strip())\n",
    "\n",
    "#getting the ctc details of the job from source code\n",
    "CTC = []\n",
    "for i in range(1,len(job_info),3):\n",
    "    CTC.append(job_info[i].strip())\n",
    "\n",
    "#making dataframe from scraped data\n",
    "jobs= pd.DataFrame({})\n",
    "jobs['title']= job_titles\n",
    "jobs['company']= company_names\n",
    "jobs['ctc']= CTC\n",
    "jobs['Apply_Date'] = apply_date\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question10- Write a python program to scrape house details from mentioned url.\n",
    "url = \"https://www.nobroker.in/property/sale/bangalore/Electronic%20City?type=BHK4&searchParam=W3sibGF0IjoxMi44NDUyMTQ1LCJsb24iOjc3LjY2MDE2OTUsInBsYWNlSWQiOiJDaElKdy1GUWQ0cHNyanNSSGZkYXpnXzhYRW8iLCJwbGFjZU5hbWUiOiJFbGVjdHJvbmljIENpdHkifV0=&propertyAge=0&radius=2.0\"\n",
    "It should include house title, location, area, emi and price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='www.nobroker.in', port=443): Max retries exceeded with url: /property/sale/bangalore/Electronic%20City?searchParam=W3sibGF0IjoxMi44NDUyMTQ1LCJsb24iOjc3LjY2MDE2OTUsInBsYWNlSWQiOiJDaElKdy1GUWQ0cHNyanNSSGZkYXpnXzhYRW8iLCJwbGFjZU5hbWUiOiJFbGVjdHJvbmljIENpdHkifV0=&radius=2.0&type=BHK4 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000206362ADF70>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             conn = connection.create_connection(\n\u001b[0m\u001b[0;32m    160\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    917\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    669\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m    977\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sock\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;31m# Add certificate verification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m         \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSocketError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m             raise NewConnectionError(\n\u001b[0m\u001b[0;32m    172\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x00000206362ADF70>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 726\u001b[1;33m             retries = retries.increment(\n\u001b[0m\u001b[0;32m    727\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.nobroker.in', port=443): Max retries exceeded with url: /property/sale/bangalore/Electronic%20City?searchParam=W3sibGF0IjoxMi44NDUyMTQ1LCJsb24iOjc3LjY2MDE2OTUsInBsYWNlSWQiOiJDaElKdy1GUWQ0cHNyanNSSGZkYXpnXzhYRW8iLCJwbGFjZU5hbWUiOiJFbGVjdHJvbmljIENpdHkifV0=&radius=2.0&type=BHK4 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000206362ADF70>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-1106f9cde177>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#sending request to get the page data and souce code from web server for scraping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://www.nobroker.in/property/sale/bangalore/Electronic%20City?searchParam=W3sibGF0IjoxMi44NDUyMTQ1LCJsb24iOjc3LjY2MDE2OTUsInBsYWNlSWQiOiJDaElKdy1GUWQ0cHNyanNSSGZkYXpnXzhYRW8iLCJwbGFjZU5hbWUiOiJFbGVjdHJvbmljIENpdHkifV0=&radius=2.0&type=BHK4\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#getting page content\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    528\u001b[0m         }\n\u001b[0;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    514\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='www.nobroker.in', port=443): Max retries exceeded with url: /property/sale/bangalore/Electronic%20City?searchParam=W3sibGF0IjoxMi44NDUyMTQ1LCJsb24iOjc3LjY2MDE2OTUsInBsYWNlSWQiOiJDaElKdy1GUWQ0cHNyanNSSGZkYXpnXzhYRW8iLCJwbGFjZU5hbWUiOiJFbGVjdHJvbmljIENpdHkifV0=&radius=2.0&type=BHK4 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000206362ADF70>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))"
     ]
    }
   ],
   "source": [
    "#sending request to get the page data and souce code from web server for scraping\n",
    "page = requests.get(\"https://www.nobroker.in/property/sale/bangalore/Electronic%20City?searchParam=W3sibGF0IjoxMi44NDUyMTQ1LCJsb24iOjc3LjY2MDE2OTUsInBsYWNlSWQiOiJDaElKdy1GUWQ0cHNyanNSSGZkYXpnXzhYRW8iLCJwbGFjZU5hbWUiOiJFbGVjdHJvbmljIENpdHkifV0=&radius=2.0&type=BHK4\")\n",
    "\n",
    "#getting page content\n",
    "soup=BeautifulSoup(page.content)\n",
    "\n",
    "#geting the titles of the flat from the source code\n",
    "flats=soup.findAll('h2',class_='heading-6 font-semi-bold nb__1AShY')\n",
    "flat_title=[ ]\n",
    "for i in flats:\n",
    "    flat_title.append(i.text)\n",
    "    \n",
    "#geting the location of the flat from the source code\n",
    "locations=soup.find_all('div',class_='nb__2CMjv')\n",
    "flat_location=[ ]\n",
    "for i in locations:\n",
    "    flat_location.append(i.text)\n",
    "\n",
    "#geting the info of the area of flat from the source code\n",
    "area=soup.find_all('div',class_='nb__3oNyC')\n",
    "flat_area=[ ]\n",
    "for i in area:\n",
    "    flat_area.append(i.text)\n",
    "            \n",
    "#geting the combine list data of the flat from the source code \n",
    "data=soup.find_all('div',class_='font-semi-bold heading-6')\n",
    "flat_data = []\n",
    "for i in data:\n",
    "    flat_data.append(i.text.replace(\"\\n\",\"\").replace('Immediately',''))\n",
    "\n",
    "#getting the emi details of the flat from source code\n",
    "emi = []\n",
    "for i in range(2,len(flat_data),3):\n",
    "    emi.append(flat_data[i].strip())\n",
    "    \n",
    "#getting the price details of the flat\n",
    "price = []\n",
    "for i in range(1,len(flat_data),3):\n",
    "    price.append(flat_data[i].strip())\n",
    "\n",
    "#making dataframe from scraped data\n",
    "flat= pd.DataFrame({})\n",
    "flat['title']= flat_title\n",
    "flat['location']= flat_location\n",
    "flat['area']= flat_area\n",
    "flat['emi'] = emi\n",
    "flat['price'] = price\n",
    "\n",
    "flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above link is expired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
